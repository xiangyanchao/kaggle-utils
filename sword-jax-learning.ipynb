{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ssh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = random.PRNGKey(0)\nx = random.normal(key, (10,))\nprint(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = 3000\nx = random.normal(key, (size, size), dtype=jnp.float32)\n%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import device_put\n\nx = np.random.normal(size=(size, size)).astype(np.float32)\nx = device_put(x)\n%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = random.normal(key, (1000000,))\n%timeit selu(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selu_jit = jit(selu)\n%timeit selu_jit(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sum_logistic(x):\n  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nx_small = jnp.arange(3.)\nderivative_fn = grad(sum_logistic)\nprint(derivative_fn(x_small))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def first_finite_differences(f, x):\n  eps = 1e-3\n  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n                   for v in jnp.eye(len(x))])\n\n\nprint(first_finite_differences(sum_logistic, x_small))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat = random.normal(key, (150, 100))\nbatched_x = random.normal(key, (10, 100))\n\ndef apply_matrix(v):\n  return jnp.dot(mat, v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naively_batched_apply_matrix(v_batched):\n  return jnp.stack([apply_matrix(v) for v in v_batched])\n\nprint('Naively batched')\n%timeit naively_batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef batched_apply_matrix(v_batched):\n  return jnp.dot(v_batched, mat.T)\n\nprint('Manually batched')\n%timeit batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef vmap_batched_apply_matrix(v_batched):\n  return vmap(apply_matrix)(v_batched)\n\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vmap(apply_matrix,(0,),0)(random.normal(key, ( 10, 100))).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(vmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = jnp.arange(3. * 4.).reshape(3, 4)\nxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import lax\nprint(vmap(lambda x: lax.psum(x, 'i'), axis_name='i')(xs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.psum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.pmap(lambda x: x ** 2)(jnp.arange(1)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\n\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\nplt.plot(x_jnp, y_jnp);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(x_jnp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = x_jnp.at[0].set(10)\nprint(x_jnp)\nprint(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.array([1, 2, 1])\ny = jnp.ones(10)\njnp.convolve(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\n\ndef norm(X):\n  X = X - X.mean(0)\n  return X / X.std(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nnorm_compiled = jit(norm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1701)\nX = jnp.array(np.random.rand(10000, 10))\nnp.allclose(norm(X), norm_compiled(X), atol=1E-6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit norm(X).block_until_ready()\n%timeit norm_compiled(X).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n# @partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\n%timeit f(jnp.array([1,2]), True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n@partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\n%timeit -n1 -r1 f(1, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  #return x.reshape((np.prod(x.shape),))\n    return x[x<0]\n\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(10)\n@jit\ndef func(i):\n    return jnp.asarray(x)[i]\n\n%timeit -r1 -n1 func(jnp.arange(4))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -r1 -n1 func(jnp.arange(4))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.arange(10)\n@jit\ndef func(x,):\n    return jnp.split(x, 2, 0)\n\nfunc(x,)\n%timeit func(x, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad(func)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.arange(10)\n# @partial(jit, static_argnums=1)\ndef func(x, axis):\n    return jnp.split(x, 2, axis)\n\nfunc(x, 0)\n%timeit func(x, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  return x.reshape((np.prod(x.shape),))\n\nx = jnp.ones((2, 3))\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((2, 4,6))\n%timeit -n100 -r1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2=np.array([[1,2],[3,4]])\n%timeit -n100 -r1  f(x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nimport jax\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jax.jit(slow_f)         # 静态编译slow_f;\n%timeit -n10 -r3 fast_f(x)\n%timeit -n10 -r3 slow_f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaxpr = jax.make_jaxpr(fast_f)\njaxpr(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unjitted_loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted_lambda(x):\n  i = 0\n  while i < 20:\n    # Don't do this!, lambda will return\n    # a function with a different hash\n    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n  return x + i\n\ndef g_inner_jitted_normal(x):\n  i = 0\n  while i < 20:\n    # this is OK, since JAX can find the\n    # cached, compiled function\n    i = jax.jit(unjitted_loop_body)(i)\n  return x + i\n\nprint(\"jit called in a loop with lambdas:\")\n%timeit -n 10 g_inner_jitted_lambda(10)\n\nprint(\"jit called in a loop with caching:\")\n%timeit -n 10 g_inner_jitted_normal(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x):\n    if x < 3:\n        return 3. * x ** 2\n    else:\n        return -4 * x\n    \nstatic_f = jax.jit(f, static_argnums=(0,))\n# jax.make_jaxpr(static_f, static_argnums=(0,))(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit static_f(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit static_f(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"static_f(2),static_f(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n# x = np.random.randn(3, 4)\n\n\n@jit\ndef f(x):\n  x = np.prod(x.shape)\n  print(x)\n  return x\n#   return x.reshape()\n\nx = jnp.ones((2, 3))\n%timeit -r1 -n1 f(x)\n%timeit -r1 -n1 f(x)\n# jax.make_jaxpr(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((2, 4))\n%timeit -r1 -n1 f(x)\n%timeit -r1 -n1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((5, 14))\n%timeit -n1 -r1 f(x)\n%timeit -n1 -r1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom jax import grad, jit\nfrom jax import lax\nfrom jax import random\nimport jax\nimport jax.numpy as jnp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n  #return x.reshape(jnp.array(x.shape).prod())\n    return jnp.array(x.shape).prod()\n\nx = jnp.ones((2, 3))\nf(x)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef impure_print_side_effect(x):\n  print(\"Executing function\")  # This is a side-effect\n  return x\nprint (\"First call: \", (impure_print_side_effect)(4.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Second call: \", (impure_print_side_effect)(5.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Third call, different type: \", (impure_print_side_effect)(jnp.array([5.])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = 0.\ndef impure_saves_global(x):\n  global g\n  g = x\n  print('x',x)\n  return x\n\n# JAX runs once the transformed function with special Traced values for arguments\nprint (\"First call: \", jit(impure_saves_global)(4.))\nprint (\"Saved global: \", g)  # Saved global has an internal JAX value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"First call: \", jit(impure_saves_global)(5.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.array([1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st=dict(even=0, odd=0)\n@jit\ndef pure_uses_internal_state(x):\n  state = st#dict(even=0, odd=0)\n  for i in range(10):\n    state['even' if i % 2 == 0 else 'odd'] += x\n  return state['even'] + state['odd']\n\nprint(jit(pure_uses_internal_state)(7.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import make_jaxpr\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.scan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lax.scan\ndef func11(arr, extra):\n    ones = jnp.ones(arr.shape)\n    def body(carry, aelems):\n        ae1, ae2 = aelems\n        return (carry + ae1 * ae2 + extra, carry)\n    return lax.scan(body, 0., (arr, ones))\nmake_jaxpr(func11)(jnp.arange(16), 5.)\n# make_jaxpr(func11)(iter(range(16)), 5.) # throws error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func11(jnp.arange(16), 5.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=jnp.arange(16)\nextra=5\nones = jnp.ones(arr.shape)\ndef body(state, aelems):\n    print('state:',state)\n    print('aelems:',aelems)\n    ae1, ae2 = aelems\n    return (state + ae1 * ae2 + extra, state)\nlax.scan(body, 0., (arr, ones))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.cond)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lax.cond\nprint('arr:',arr)\n# lax.cond(arr%2==0, lambda x: x[0]+1, lambda x: x[0]-1, (arr, ones))\narr%2==0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b=arr%2==0\nprint(b)\njnp.where(b,size=40,fill_value=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numpy_array = np.zeros((3,3), dtype=np.float32)\nprint(\"original array:\")\nprint(numpy_array)\n\n# In place, mutating update\nnumpy_array[1, :] = 1.0\nprint(\"updated array:\")\nprint(numpy_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=jnp.array((3,4))\narr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = random.normal(key, (3,4))\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.at[jnp.where(x>0.2)].set(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\ndef my_log(x):\n  return jnp.where(x > 0., jnp.log(x+1e-2), jnp.log(x+1e-3))\n\njax.grad(my_log)(0.) # ==> NaN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.make_jaxpr(jax.grad(my_log))(0.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.experimental import io_callback\nfrom functools import partial\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n#   print(f'generating {x.dtype}{x}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\nnumpy_random_like(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(io_callback)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax_array = jnp.zeros((3,3), dtype=jnp.float32)\nupdated_array = jax_array.at[1, :].set(1.0)\nprint(\"updated array:\\n\", updated_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"original array:\")\njax_array = jnp.ones((5, 6))\nprint(jax_array)\n\nnew_jax_array = jax_array.at[::2, 3:].add(7.)\nprint(\"new array post-addition:\")\nprint(new_jax_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.arange(10.0).at[11].set(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef permissive_sum(x):\n  return jnp.sum(x)\n\nx = jnp.array(list(range(10)))\npermissive_sum(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit permissive_sum(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(random.split)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x):\n    return  jnp.log(x)\n\nx=jax.random.normal(key,(1000,))\nprint(f(x).shape)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n    return  jnp.log(x)\n\nx=jax.random.normal(key,(1000,))\nprint(f(x).shape)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n  for i in range(3):\n    x = 2 * x\n  return x\n\nprint(f(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.prod(jnp.array(x.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef g(x):\n  s=np.prod(np.array(x.shape))\n  print(s)\n  return jnp.ones(s)\n\n%timeit -n1 -r1 print(g(jnp.array([[1., 2., 3.]])))\n%timeit -n1 -r1 g(jnp.array([[1., 2., 3.]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 print(g(jnp.array([1., 2., 3.,4])))\n%timeit -n1 -r1 g(jnp.array([1., 2., 3.,4]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jit,static_argnums=(1,))\ndef f(x, n):\n  y = 0.\n  for i in range(n):\n    y = y + x[i]\n  return y\n\n#f = jit(f, static_argnums=(1,))\na=jnp.array([2., 3., 4.])\nf(a, 2)\n%timeit f(a, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x, y):\n   a = x * y\n   b = (x + y) / (x - y)\n   c = a + 2\n   return a + b * c\n\n\nx = jnp.array([2., 0.])\n\ny = jnp.array([3., 0.])\n\nf(x, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef func1(x):\n  return jnp.tile(x, 10) * 0.5\n\ndef func2(x):\n  y = func1(x)\n  return y, jnp.tile(x, 10) + 1\n\nx = jax.random.normal(jax.random.PRNGKey(42), (1000, 1000))\ny, z = func2(x)\n\nz.block_until_ready()\n\njax.profiler.save_device_memory_profile(\"memory.prof\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -alh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pprof \n!go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  jax.debug.print(\"🤯 {x} 🤯\", x=x)\n  y = jnp.sin(x)\n  jax.debug.breakpoint()\n  jax.debug.print(\"🤯 {y} 🤯\", y=y)\n  return y\n\nf(2.)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nfrom jax import lax\ndef breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n  def true_fn(x):\n    pass\n  def false_fn(x):\n    jax.debug.breakpoint()\n  lax.cond(is_finite, true_fn, false_fn, x)\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\nf(2., 0.) # ==> Pauses during execution!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = jnp.arange(3.)\n\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"y: {}\", y)\n  return y\nprint('v1',jax.vmap(f)(xs))\n# Prints: x: 0.0\n#         x: 1.0\n#         x: 2.0\n#         y: 0.0\n#         y: 0.841471\n#         y: 0.9092974\nprint('v2',jax.lax.map(f, xs))\n# Prints: x: 0.0\n#         y: 0.0\n#         x: 1.0\n#         y: 0.841471\n#         x: 2.0\n#         y: 0.9092974","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n  checkify.check(i >= 0, f\"index needs to be non-negative, got {i}\")\n  y = x[i]\n  z = jnp.sin(y)\n  return z\n\njittable_f = checkify.checkify(f)\n\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -2)\nprint(err.get())\n# >> index needs to be non-negative, got -2! (check failed at <...>:6 (f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <…>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)\n\nerr, z = checked_f(jnp.array([5, 1]), 0)\nerr.throw()  # if no error occurred, throw does nothing!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  return jax.pure_callback(f_host, x, x)\n\nx = jnp.arange(5.0)\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.jit(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.vmap(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def body_fun(_, x):\n  return _+1, f(x)\njax.lax.scan(body_fun, 0, jnp.arange(5.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%xmode minimal\njax.grad(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.asarray(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.array(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nfrom functools import partial\n@partial(jit, static_argnums=1)\ndef func(x, axis):\n    return x.min(axis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func(jnp.arange(4), 0)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A helper function to randomly initialize weights and biases\n# for a dense neural network layer\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\n# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 10\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.PRNGKey(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  \n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits)  #log(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logsumexp(np.array([-2000,-1.,0,1,20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lx(logits):\n    return logits - logsumexp(logits)\nlx(jnp.array([-2000,-1.,0,1,20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This works on single examples\nrandom_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\npreds = predict(params, random_flattened_image)\nprint(preds.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Doesn't work with a batch\nrandom_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\ntry:\n  preds = predict(params, random_flattened_images)\nexcept TypeError:\n  print('Invalid shapes!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's upgrade it to handle batches using `vmap`\n\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=jnp.array([1,2,3])\nk=9\njnp.array(x[:, None] == jnp.arange(k), jnp.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n  \ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# Ensure TF does not see GPU and grab all GPU memory.\ntf.config.set_visible_devices([], device_type='GPU')\n\nimport tensorflow_datasets as tfds\n\ndata_dir = '/tmp/tfds'\n\n# Fetch full datasets for evaluation\n# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\nmnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\nmnist_data = tfds.as_numpy(mnist_data)\ntrain_data, test_data = mnist_data['train'], mnist_data['test']\nnum_labels = info.features['label'].num_classes\nh, w, c = info.features['image'].shape\nnum_pixels = h * w * c\n\n# Full train set\ntrain_images, train_labels = train_data['image'], train_data['label']\ntrain_images = jnp.reshape(train_images, (len(train_images), num_pixels))\ntrain_labels = one_hot(train_labels, num_labels)\n\n# Full test set\ntest_images, test_labels = test_data['image'], test_data['label']\ntest_images = jnp.reshape(test_images, (len(test_images), num_pixels))\ntest_labels = one_hot(test_labels, num_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train:', train_images.shape, train_labels.shape)\nprint('Test:', test_images.shape, test_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef get_train_batches():\n  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n  # You can build up an arbitrary tf.data input pipeline\n  ds = ds.batch(batch_size).prefetch(1)\n  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n  return tfds.as_numpy(ds)\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in get_train_batches():\n    x = jnp.reshape(x, (len(x), num_pixels))\n    y = one_hot(y, num_labels)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[[e.shape for e in p] for p in params]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flax import linen as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-core","metadata":{"execution":{"iopub.status.busy":"2023-08-26T16:36:43.180092Z","iopub.execute_input":"2023-08-26T16:36:43.180701Z","iopub.status.idle":"2023-08-26T16:36:59.834054Z","shell.execute_reply.started":"2023-08-26T16:36:43.180659Z","shell.execute_reply":"2023-08-26T16:36:59.832639Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-core\n  Downloading keras_core-0.1.5-py3-none-any.whl (924 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m924.6/924.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.23.5)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.4.2)\nCollecting namex (from keras-core)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.0)\nInstalling collected packages: namex, keras-core\nSuccessfully installed keras-core-0.1.5 namex-0.0.7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# JAX","metadata":{}},{"cell_type":"code","source":"import sys\nif '/kaggle/input/kaggle-utils' not in sys.path:\n    sys.path.append('/kaggle/input/kaggle-utils')\nimport utils\nimport importlib \nimportlib.reload(utils)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T16:36:59.837330Z","iopub.execute_input":"2023-08-26T16:36:59.837753Z","iopub.status.idle":"2023-08-26T16:37:09.963556Z","shell.execute_reply.started":"2023-08-26T16:36:59.837711Z","shell.execute_reply":"2023-08-26T16:37:09.962165Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Using JAX backend.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<module 'utils' from '/kaggle/input/kaggle-utils/utils.py'>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport jax\nimport numpy as np\nimport tensorflow as tf\nimport keras_core as keras\n\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import Mesh\nfrom jax.sharding import NamedSharding\nfrom jax.sharding import PartitionSpec\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nkey = random.PRNGKey(0)\n\n\ndef get_model():\n    # Make a simple 2-layer densely-connected neural network.\n    inputs = keras.Input(shape=(28,28,1))\n    x = keras.layers.Reshape((28*28,))(inputs)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    return model\n\n\ndef get_datasets():\n    # Load the data and split it between train and test sets\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Scale images to the [0, 1] range\n    x_train = x_train.astype(\"float32\")\n    x_test = x_test.astype(\"float32\")\n    # Make sure images have shape (28, 28, 1)\n    x_train = np.expand_dims(x_train, -1)\n    x_test = np.expand_dims(x_test, -1)\n    print(\"x_train shape:\", x_train.shape)\n    print(x_train.shape[0], \"train samples\")\n    print(x_test.shape[0], \"test samples\")\n\n    # Create TF Datasets\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    return train_data, eval_data\n\nnum_epochs = 20\nbatch_size = 64 * 8\n\ntrain_data, eval_data = get_datasets()\ntrain_data = train_data.batch(batch_size, drop_remainder=True)\n\ndevices = utils.get_devices()\ntest_shard_data=random.normal(key, (batch_size, 128*128), dtype=jnp.float32)\ndata_sharding = utils.get_data_sharding(devices,test_shard_data)\n\nmodel = get_model()\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = keras.optimizers.Adam(1e-3)\nutils.set_model_suit(model,loss_fn,optimizer)\n\n(one_batch, one_batch_labels) = next(iter(train_data))\nmodel.build(one_batch)\noptimizer.build(model.trainable_variables)\nmodel.summary()\n\ntrain_state = utils.get_replicated_train_state(devices,model,optimizer)\n\n# Custom training loop\nfor epoch in range(num_epochs):\n    import time\n    start=time.time()\n    data_iter = iter(train_data)\n    for data in data_iter:\n        x, y = data\n        sharded_x = jax.device_put(x.numpy(), data_sharding)\n        loss_value, train_state = utils.train_step(train_state, sharded_x, y.numpy())\n    print(\"Epoch\", epoch, \"loss:\", loss_value,'cost:',time.time()-start)\n\n# Post-processing model state update to write them back into the model\ntrainable_variables, non_trainable_variables, optimizer_variables = train_state\nfor variable, value in zip(model.trainable_variables, trainable_variables):\n    variable.assign(value)\nfor variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n    variable.assign(value)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T16:38:52.419358Z","iopub.execute_input":"2023-08-26T16:38:52.420077Z","iopub.status.idle":"2023-08-26T16:39:51.113345Z","shell.execute_reply.started":"2023-08-26T16:38:52.420044Z","shell.execute_reply":"2023-08-26T16:39:51.112185Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"x_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nRunning on 1 devices: [gpu(id=0)]\nData sharding...\ntest_shard_data: (512, 16384)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                     \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mGPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                                      \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n\u001b[38;2;255;255;255;48;2;57;59;121m                                                                                \u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                     GPU 0                                      </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                                                </span>\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Data sharding over!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)               │          \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │    \u001b[38;5;34m200,960\u001b[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │     \u001b[38;5;34m65,792\u001b[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                │      \u001b[38;5;34m2,570\u001b[0m │\n└─────────────────────────────────┴───────────────────────────┴────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │    <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n└─────────────────────────────────┴───────────────────────────┴────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 0 loss: 0.9693474 cost: 51.677998304367065\nEpoch 1 loss: 0.47046852 cost: 0.4756608009338379\nEpoch 2 loss: 0.25392938 cost: 0.5402429103851318\nEpoch 3 loss: 0.11509393 cost: 0.29418468475341797\nEpoch 4 loss: 0.12776947 cost: 0.27455568313598633\nEpoch 5 loss: 0.107299134 cost: 0.30698108673095703\nEpoch 6 loss: 0.043443 cost: 0.2800438404083252\nEpoch 7 loss: 0.104660384 cost: 0.29596829414367676\nEpoch 8 loss: 0.01719129 cost: 0.2734522819519043\nEpoch 9 loss: 0.027290137 cost: 0.3023505210876465\nEpoch 10 loss: 0.0095254015 cost: 0.2686445713043213\nEpoch 11 loss: 0.032697883 cost: 0.2578747272491455\nEpoch 12 loss: 0.029653922 cost: 0.26798057556152344\nEpoch 13 loss: 0.035819754 cost: 0.33320116996765137\nEpoch 14 loss: 0.023044964 cost: 0.26282453536987305\nEpoch 15 loss: 0.0038949219 cost: 0.25931549072265625\nEpoch 16 loss: 0.13145132 cost: 0.31117773056030273\nEpoch 17 loss: 0.01595957 cost: 0.2969954013824463\nEpoch 18 loss: 0.054846402 cost: 0.2645421028137207\nEpoch 19 loss: 0.060465656 cost: 0.2995941638946533\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}