{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = random.PRNGKey(0)\nx = random.normal(key, (10,))\nprint(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = 3000\nx = random.normal(key, (size, size), dtype=jnp.float32)\n%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import device_put\n\nx = np.random.normal(size=(size, size)).astype(np.float32)\nx = device_put(x)\n%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit jnp.dot(x, x.T).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = random.normal(key, (1000000,))\n%timeit selu(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selu_jit = jit(selu)\n%timeit selu_jit(x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sum_logistic(x):\n  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nx_small = jnp.arange(3.)\nderivative_fn = grad(sum_logistic)\nprint(derivative_fn(x_small))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def first_finite_differences(f, x):\n  eps = 1e-3\n  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n                   for v in jnp.eye(len(x))])\n\n\nprint(first_finite_differences(sum_logistic, x_small))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat = random.normal(key, (150, 100))\nbatched_x = random.normal(key, (10, 100))\n\ndef apply_matrix(v):\n  return jnp.dot(mat, v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naively_batched_apply_matrix(v_batched):\n  return jnp.stack([apply_matrix(v) for v in v_batched])\n\nprint('Naively batched')\n%timeit naively_batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef batched_apply_matrix(v_batched):\n  return jnp.dot(v_batched, mat.T)\n\nprint('Manually batched')\n%timeit batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef vmap_batched_apply_matrix(v_batched):\n  return vmap(apply_matrix)(v_batched)\n\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vmap(apply_matrix,(0,),0)(random.normal(key, ( 10, 100))).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(vmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = jnp.arange(3. * 4.).reshape(3, 4)\nxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import lax\nprint(vmap(lambda x: lax.psum(x, 'i'), axis_name='i')(xs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.psum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.pmap(lambda x: x ** 2)(jnp.arange(1)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\n\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\nplt.plot(x_jnp, y_jnp);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(x_jnp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = x_jnp.at[0].set(10)\nprint(x_jnp)\nprint(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.array([1, 2, 1])\ny = jnp.ones(10)\njnp.convolve(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\n\ndef norm(X):\n  X = X - X.mean(0)\n  return X / X.std(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nnorm_compiled = jit(norm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1701)\nX = jnp.array(np.random.rand(10000, 10))\nnp.allclose(norm(X), norm_compiled(X), atol=1E-6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit norm(X).block_until_ready()\n%timeit norm_compiled(X).block_until_ready()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n# @partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\n%timeit f(jnp.array([1,2]), True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n@partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\n%timeit -n1 -r1 f(1, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 f(1, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  #return x.reshape((np.prod(x.shape),))\n    return x[x<0]\n\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(10)\n@jit\ndef func(i):\n    return jnp.asarray(x)[i]\n\n%timeit -r1 -n1 func(jnp.arange(4))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -r1 -n1 func(jnp.arange(4))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.arange(10)\n@jit\ndef func(x,):\n    return jnp.split(x, 2, 0)\n\nfunc(x,)\n%timeit func(x, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad(func)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.arange(10)\n# @partial(jit, static_argnums=1)\ndef func(x, axis):\n    return jnp.split(x, 2, axis)\n\nfunc(x, 0)\n%timeit func(x, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  return x.reshape((np.prod(x.shape),))\n\nx = jnp.ones((2, 3))\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((2, 4,6))\n%timeit -n100 -r1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2=np.array([[1,2],[3,4]])\n%timeit -n100 -r1  f(x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nimport jax\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jax.jit(slow_f)         # é™æ€ç¼–è¯‘slow_f;\n%timeit -n10 -r3 fast_f(x)\n%timeit -n10 -r3 slow_f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaxpr = jax.make_jaxpr(fast_f)\njaxpr(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unjitted_loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted_lambda(x):\n  i = 0\n  while i < 20:\n    # Don't do this!, lambda will return\n    # a function with a different hash\n    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n  return x + i\n\ndef g_inner_jitted_normal(x):\n  i = 0\n  while i < 20:\n    # this is OK, since JAX can find the\n    # cached, compiled function\n    i = jax.jit(unjitted_loop_body)(i)\n  return x + i\n\nprint(\"jit called in a loop with lambdas:\")\n%timeit -n 10 g_inner_jitted_lambda(10)\n\nprint(\"jit called in a loop with caching:\")\n%timeit -n 10 g_inner_jitted_normal(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x):\n    if x < 3:\n        return 3. * x ** 2\n    else:\n        return -4 * x\n    \nstatic_f = jax.jit(f, static_argnums=(0,))\n# jax.make_jaxpr(static_f, static_argnums=(0,))(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit static_f(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit static_f(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"static_f(2),static_f(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n# x = np.random.randn(3, 4)\n\n\n@jit\ndef f(x):\n  x = np.prod(x.shape)\n  print(x)\n  return x\n#   return x.reshape()\n\nx = jnp.ones((2, 3))\n%timeit -r1 -n1 f(x)\n%timeit -r1 -n1 f(x)\n# jax.make_jaxpr(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((2, 4))\n%timeit -r1 -n1 f(x)\n%timeit -r1 -n1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = jnp.ones((5, 14))\n%timeit -n1 -r1 f(x)\n%timeit -n1 -r1 f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom jax import grad, jit\nfrom jax import lax\nfrom jax import random\nimport jax\nimport jax.numpy as jnp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n  #return x.reshape(jnp.array(x.shape).prod())\n    return jnp.array(x.shape).prod()\n\nx = jnp.ones((2, 3))\nf(x)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef impure_print_side_effect(x):\n  print(\"Executing function\")  # This is a side-effect\n  return x\nprint (\"First call: \", (impure_print_side_effect)(4.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Second call: \", (impure_print_side_effect)(5.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Third call, different type: \", (impure_print_side_effect)(jnp.array([5.])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = 0.\ndef impure_saves_global(x):\n  global g\n  g = x\n  print('x',x)\n  return x\n\n# JAX runs once the transformed function with special Traced values for arguments\nprint (\"First call: \", jit(impure_saves_global)(4.))\nprint (\"Saved global: \", g)  # Saved global has an internal JAX value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"First call: \", jit(impure_saves_global)(5.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.array([1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st=dict(even=0, odd=0)\n@jit\ndef pure_uses_internal_state(x):\n  state = st#dict(even=0, odd=0)\n  for i in range(10):\n    state['even' if i % 2 == 0 else 'odd'] += x\n  return state['even'] + state['odd']\n\nprint(jit(pure_uses_internal_state)(7.))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import make_jaxpr\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.scan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lax.scan\ndef func11(arr, extra):\n    ones = jnp.ones(arr.shape)\n    def body(carry, aelems):\n        ae1, ae2 = aelems\n        return (carry + ae1 * ae2 + extra, carry)\n    return lax.scan(body, 0., (arr, ones))\nmake_jaxpr(func11)(jnp.arange(16), 5.)\n# make_jaxpr(func11)(iter(range(16)), 5.) # throws error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func11(jnp.arange(16), 5.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=jnp.arange(16)\nextra=5\nones = jnp.ones(arr.shape)\ndef body(state, aelems):\n    print('state:',state)\n    print('aelems:',aelems)\n    ae1, ae2 = aelems\n    return (state + ae1 * ae2 + extra, state)\nlax.scan(body, 0., (arr, ones))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(lax.cond)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lax.cond\nprint('arr:',arr)\n# lax.cond(arr%2==0, lambda x: x[0]+1, lambda x: x[0]-1, (arr, ones))\narr%2==0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b=arr%2==0\nprint(b)\njnp.where(b,size=40,fill_value=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numpy_array = np.zeros((3,3), dtype=np.float32)\nprint(\"original array:\")\nprint(numpy_array)\n\n# In place, mutating update\nnumpy_array[1, :] = 1.0\nprint(\"updated array:\")\nprint(numpy_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=jnp.array((3,4))\narr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = random.normal(key, (3,4))\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.at[jnp.where(x>0.2)].set(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\ndef my_log(x):\n  return jnp.where(x > 0., jnp.log(x+1e-2), jnp.log(x+1e-3))\n\njax.grad(my_log)(0.) # ==> NaN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.make_jaxpr(jax.grad(my_log))(0.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.experimental import io_callback\nfrom functools import partial\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n#   print(f'generating {x.dtype}{x}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\nnumpy_random_like(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(io_callback)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax_array = jnp.zeros((3,3), dtype=jnp.float32)\nupdated_array = jax_array.at[1, :].set(1.0)\nprint(\"updated array:\\n\", updated_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"original array:\")\njax_array = jnp.ones((5, 6))\nprint(jax_array)\n\nnew_jax_array = jax_array.at[::2, 3:].add(7.)\nprint(\"new array post-addition:\")\nprint(new_jax_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.arange(10.0).at[11].set(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef permissive_sum(x):\n  return jnp.sum(x)\n\nx = jnp.array(list(range(10)))\npermissive_sum(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit permissive_sum(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(random.split)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x):\n    return  jnp.log(x)\n\nx=jax.random.normal(key,(1000,))\nprint(f(x).shape)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n    return  jnp.log(x)\n\nx=jax.random.normal(key,(1000,))\nprint(f(x).shape)\n%timeit f(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x):\n  for i in range(3):\n    x = 2 * x\n  return x\n\nprint(f(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.prod(jnp.array(x.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef g(x):\n  s=np.prod(np.array(x.shape))\n  print(s)\n  return jnp.ones(s)\n\n%timeit -n1 -r1 print(g(jnp.array([[1., 2., 3.]])))\n%timeit -n1 -r1 g(jnp.array([[1., 2., 3.]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n1 -r1 print(g(jnp.array([1., 2., 3.,4])))\n%timeit -n1 -r1 g(jnp.array([1., 2., 3.,4]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jit,static_argnums=(1,))\ndef f(x, n):\n  y = 0.\n  for i in range(n):\n    y = y + x[i]\n  return y\n\n#f = jit(f, static_argnums=(1,))\na=jnp.array([2., 3., 4.])\nf(a, 2)\n%timeit f(a, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jit\ndef f(x, y):\n   a = x * y\n   b = (x + y) / (x - y)\n   c = a + 2\n   return a + b * c\n\n\nx = jnp.array([2., 0.])\n\ny = jnp.array([3., 0.])\n\nf(x, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef func1(x):\n  return jnp.tile(x, 10) * 0.5\n\ndef func2(x):\n  y = func1(x)\n  return y, jnp.tile(x, 10) + 1\n\nx = jax.random.normal(jax.random.PRNGKey(42), (1000, 1000))\ny, z = func2(x)\n\nz.block_until_ready()\n\njax.profiler.save_device_memory_profile(\"memory.prof\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -alh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pprof \n!go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  jax.debug.print(\"ðŸ¤¯ {x} ðŸ¤¯\", x=x)\n  y = jnp.sin(x)\n  jax.debug.breakpoint()\n  jax.debug.print(\"ðŸ¤¯ {y} ðŸ¤¯\", y=y)\n  return y\n\nf(2.)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nfrom jax import lax\ndef breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n  def true_fn(x):\n    pass\n  def false_fn(x):\n    jax.debug.breakpoint()\n  lax.cond(is_finite, true_fn, false_fn, x)\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\nf(2., 0.) # ==> Pauses during execution!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = jnp.arange(3.)\n\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"y: {}\", y)\n  return y\nprint('v1',jax.vmap(f)(xs))\n# Prints: x: 0.0\n#         x: 1.0\n#         x: 2.0\n#         y: 0.0\n#         y: 0.841471\n#         y: 0.9092974\nprint('v2',jax.lax.map(f, xs))\n# Prints: x: 0.0\n#         y: 0.0\n#         x: 1.0\n#         y: 0.841471\n#         x: 2.0\n#         y: 0.9092974","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n  checkify.check(i >= 0, f\"index needs to be non-negative, got {i}\")\n  y = x[i]\n  z = jnp.sin(y)\n  return z\n\njittable_f = checkify.checkify(f)\n\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -2)\nprint(err.get())\n# >> index needs to be non-negative, got -2! (check failed at <...>:6 (f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <â€¦>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)\n\nerr, z = checked_f(jnp.array([5, 1]), 0)\nerr.throw()  # if no error occurred, throw does nothing!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  return jax.pure_callback(f_host, x, x)\n\nx = jnp.arange(5.0)\nf(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.jit(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.vmap(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def body_fun(_, x):\n  return _+1, f(x)\njax.lax.scan(body_fun, 0, jnp.arange(5.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%xmode minimal\njax.grad(f)(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.asarray(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jnp.array(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax import jit\nfrom functools import partial\n@partial(jit, static_argnums=1)\ndef func(x, axis):\n    return x.min(axis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func(jnp.arange(4), 0)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A helper function to randomly initialize weights and biases\n# for a dense neural network layer\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\n# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 10\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.PRNGKey(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  \n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits)  #log(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logsumexp(np.array([-2000,-1.,0,1,20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lx(logits):\n    return logits - logsumexp(logits)\nlx(jnp.array([-2000,-1.,0,1,20]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This works on single examples\nrandom_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\npreds = predict(params, random_flattened_image)\nprint(preds.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Doesn't work with a batch\nrandom_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\ntry:\n  preds = predict(params, random_flattened_images)\nexcept TypeError:\n  print('Invalid shapes!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's upgrade it to handle batches using `vmap`\n\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=jnp.array([1,2,3])\nk=9\njnp.array(x[:, None] == jnp.arange(k), jnp.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n  \ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n# Ensure TF does not see GPU and grab all GPU memory.\ntf.config.set_visible_devices([], device_type='GPU')\n\nimport tensorflow_datasets as tfds\n\ndata_dir = '/tmp/tfds'\n\n# Fetch full datasets for evaluation\n# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\nmnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\nmnist_data = tfds.as_numpy(mnist_data)\ntrain_data, test_data = mnist_data['train'], mnist_data['test']\nnum_labels = info.features['label'].num_classes\nh, w, c = info.features['image'].shape\nnum_pixels = h * w * c\n\n# Full train set\ntrain_images, train_labels = train_data['image'], train_data['label']\ntrain_images = jnp.reshape(train_images, (len(train_images), num_pixels))\ntrain_labels = one_hot(train_labels, num_labels)\n\n# Full test set\ntest_images, test_labels = test_data['image'], test_data['label']\ntest_images = jnp.reshape(test_images, (len(test_images), num_pixels))\ntest_labels = one_hot(test_labels, num_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train:', train_images.shape, train_labels.shape)\nprint('Test:', test_images.shape, test_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef get_train_batches():\n  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n  # You can build up an arbitrary tf.data input pipeline\n  ds = ds.batch(batch_size).prefetch(1)\n  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n  return tfds.as_numpy(ds)\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in get_train_batches():\n    x = jnp.reshape(x, (len(x), num_pixels))\n    y = one_hot(y, num_labels)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[[e.shape for e in p] for p in params]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flax import linen as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-core","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# JAX","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport jax\nimport numpy as np\nimport tensorflow as tf\nimport keras_core as keras\n\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import Mesh\nfrom jax.sharding import NamedSharding\nfrom jax.sharding import PartitionSpec as P\n\n\ndef get_model():\n    # Make a simple 2-layer densely-connected neural network.\n    inputs = keras.Input(shape=(28,28,1))\n    x = keras.layers.Reshape((28*28,))(inputs)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    return model\n\n\ndef get_datasets():\n    # Load the data and split it between train and test sets\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Scale images to the [0, 1] range\n    x_train = x_train.astype(\"float32\")\n    x_test = x_test.astype(\"float32\")\n    # Make sure images have shape (28, 28, 1)\n    x_train = np.expand_dims(x_train, -1)\n    x_test = np.expand_dims(x_test, -1)\n    print(\"x_train shape:\", x_train.shape)\n    print(x_train.shape[0], \"train samples\")\n    print(x_test.shape[0], \"test samples\")\n\n    # Create TF Datasets\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    return train_data, eval_data\n\nnum_epochs = 2\nbatch_size = 64 \n\ntrain_data, eval_data = get_datasets()\ntrain_data = train_data.batch(batch_size, drop_remainder=True)\n\nmodel = get_model()\noptimizer = keras.optimizers.Adam(1e-3)\nloss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Initialize all state with .build()\n(one_batch, one_batch_labels) = next(iter(train_data))\nmodel.build(one_batch)\noptimizer.build(model.trainable_variables)\n\n\n# This is the loss function that will be differentiated.\n# Keras provides a pure functional forward pass: model.stateless_call\ndef compute_loss(trainable_variables, non_trainable_variables, x, y):\n    y_pred, updated_non_trainable_variables = model.stateless_call(\n        trainable_variables, non_trainable_variables, x\n    )\n    loss_value = loss(y, y_pred)\n    return loss_value, updated_non_trainable_variables\n\n\n# Function to compute gradients\ncompute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n\n\n# Training step, Keras provides a pure functional optimizer.stateless_apply\n@jax.jit\ndef train_step(train_state, x, y):\n    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n    (loss_value, non_trainable_variables), grads = compute_gradients(\n        trainable_variables, non_trainable_variables, x, y\n    )\n\n    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n        optimizer_variables, grads, trainable_variables\n    )\n\n    return loss_value, (\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n    )\n\n\n# Replicate the model and optimizer variable on all devices\ndef get_replicated_train_state(devices):\n    # All variables will be replicated on all devices\n    var_mesh = Mesh(devices, axis_names=(\"_\"))\n    # In NamedSharding, axes not mentioned are replicated (all axes here)\n    var_replication = NamedSharding(var_mesh, P())\n\n    # Apply the distribution settings to the model variables\n    trainable_variables     = jax.device_put(model.trainable_variables,     var_replication)\n    non_trainable_variables = jax.device_put(model.non_trainable_variables, var_replication)\n    optimizer_variables     = jax.device_put(optimizer.variables,           var_replication)\n\n    # Combine all state in a tuple\n    return (trainable_variables, non_trainable_variables, optimizer_variables)\n\n\nnum_devices = len(jax.local_devices())\nprint(f\"Running on {num_devices} devices: {jax.local_devices()}\")\ndevices = mesh_utils.create_device_mesh((num_devices,))\n\n# Data will be split along the batch axis\ndata_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\ndata_sharding = NamedSharding(\n    data_mesh,\n    P(\n        \"batch\",\n    ),\n)  # naming axes of the sharded partition\n\n# Display data sharding\nx, y = next(iter(train_data))\nsharded_x = jax.device_put(x.numpy(), data_sharding)\nprint(\"Data sharding\")\njax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28 * 28]))\n\ntrain_state = get_replicated_train_state(devices)\n\n# Custom training loop\nfor epoch in range(4):\n    import time\n    start=time.time()\n    data_iter = iter(train_data)\n    for data in data_iter:\n        x, y = data\n        sharded_x = jax.device_put(x.numpy(), data_sharding)\n        loss_value, train_state = train_step(train_state, sharded_x, y.numpy())\n    print(\"Epoch\", epoch, \"loss:\", loss_value,'cost:',time.time()-start)\n\n# Post-processing model state update to write them back into the model\ntrainable_variables, non_trainable_variables, optimizer_variables = train_state\nfor variable, value in zip(model.trainable_variables, trainable_variables):\n    variable.assign(value)\nfor variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n    variable.assign(value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tensorflow","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport tensorflow as tf\nimport keras_core as keras\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local')\n# strategy = tf.distribute.TPUStrategy(tpu)\nstrategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_compiled_model():\n    # Make a simple 2-layer densely-connected neural network.\n    inputs = keras.Input(shape=(784,))\n    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    )\n    return model\n\n\ndef get_dataset():\n    batch_size = 64 * 8 #strategy.num_replicas_in_sync\n    num_val_samples = 10000\n\n    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Preprocess the data (these are Numpy arrays)\n    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n    y_train = y_train.astype(\"float32\")\n    y_test = y_test.astype(\"float32\")\n\n    # Reserve num_val_samples samples for validation\n    x_val = x_train[-num_val_samples:]\n    y_val = y_train[-num_val_samples:]\n    x_train = x_train[:-num_val_samples]\n    y_train = y_train[:-num_val_samples]\n    return (\n        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n    )\n\n\n\n# Open a strategy scope.\nwith strategy.scope():\n    # Everything that creates variables should be under the strategy scope.\n    # In general this is only model construction & `compile()`.\n    model = get_compiled_model()\n\n    # Train the model on all available devices.\n    train_dataset, val_dataset, test_dataset = get_dataset()\n    model.fit(train_dataset, epochs=4)\n\n    # Test the model on all available devices.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# torch","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\n\nimport torch\nimport numpy as np\nimport keras_core as keras\n\n\n\ndef get_model():\n    # Make a simple 2-layer densely-connected neural network.\n    inputs = keras.Input(shape=(28,28,1))\n    x = keras.layers.Reshape((28*28,))(inputs)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    return model\n\n\ndef get_dataset():\n    # Load the data and split it between train and test sets\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Scale images to the [0, 1] range\n    x_train = x_train.astype(\"float32\")\n    x_test = x_test.astype(\"float32\")\n    # Make sure images have shape (28, 28, 1)\n    x_train = np.expand_dims(x_train, -1)\n    x_test = np.expand_dims(x_test, -1)\n    print(\"x_train shape:\", x_train.shape)\n\n    # Create a TensorDataset\n    dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(x_train), torch.from_numpy(y_train)\n    )\n    return dataset\n\ndef train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n    for epoch in range(num_epochs):\n        import time\n        start=time.time()\n        running_loss = 0.0\n        running_loss_count = 0\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\n            inputs = inputs.cuda(non_blocking=True)\n            targets = targets.cuda(non_blocking=True)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            running_loss_count += 1\n\n        # Print loss statistics\n        print(\n            f\"Epoch {epoch + 1}/{num_epochs}, \"\n            f\"Loss: {running_loss / running_loss_count}, \"\n            f\"Cost: {time.time()-start}\"\n        )\nnum_gpu = torch.cuda.device_count()\nnum_epochs = 2\nbatch_size = 64 *8\nprint(f\"Running on {num_gpu} GPUs\")\n\n\ndef setup_device(current_gpu_index, num_gpus):\n    # Device setup\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"56492\"\n    device = torch.device(\"cuda:{}\".format(current_gpu_index))\n    torch.distributed.init_process_group(\n        backend=\"nccl\",\n        init_method=\"env://\",\n        world_size=num_gpus,\n        rank=current_gpu_index,\n    )\n    torch.cuda.set_device(device)\n\n\ndef cleanup():\n    torch.distributed.destroy_process_group()\n\n\ndef prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):\n    sampler = torch.utils.data.distributed.DistributedSampler(\n        dataset,\n        num_replicas=num_gpus,\n        rank=current_gpu_index,\n        shuffle=False,\n    )\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        sampler=sampler,\n        batch_size=batch_size,\n        shuffle=False,\n    )\n    return dataloader\n\n\ndef per_device_launch_fn(current_gpu_index, num_gpu):\n    # Setup the process groups\n    setup_device(current_gpu_index, num_gpu)\n\n    dataset = get_dataset()\n    model = get_model()\n\n    # prepare the dataloader\n    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n\n    # Instantiate the torch optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Instantiate the torch loss function\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    # Put model on device\n    model = model.to(current_gpu_index)\n    ddp_model = torch.nn.parallel.DistributedDataParallel(\n        model, device_ids=[current_gpu_index], output_device=current_gpu_index\n    )\n\n    train_model(ddp_model, dataloader, 4, optimizer, loss_fn)\n\n    cleanup()\n    \nper_device_launch_fn(0,1)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T13:45:10.482975Z","iopub.execute_input":"2023-08-26T13:45:10.483349Z","iopub.status.idle":"2023-08-26T13:45:14.309151Z","shell.execute_reply.started":"2023-08-26T13:45:10.483315Z","shell.execute_reply":"2023-08-26T13:45:14.308118Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Running on 1 GPUs\nx_train shape: (60000, 28, 28, 1)\nEpoch 1/4, Loss: 5.487846383603952, Cost: 0.7879757881164551\nEpoch 2/4, Loss: 0.6967917170565007, Cost: 0.7826569080352783\nEpoch 3/4, Loss: 0.3728914860699136, Cost: 0.7845509052276611\nEpoch 4/4, Loss: 0.21847973126223533, Cost: 1.052258014678955\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}